import logging
import json
from typing import Literal
import re

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END, START
from langgraph.checkpoint.memory import logger
from langchain_neo4j.graphs.neo4j_graph import Neo4jGraph
from langgraph.config import get_stream_writer

from utils.utils import checkpointer, process_streaming_content
from src.chat_llm import ChatTongyiLLM, build_tongyi_llm, get_api_key_from_config
from sub_graph.subgraph_states import SubgraphState, SubRouter
from prompts.prompt_templates import SUBGRAPH_ROUTER_SYSTEM_PROMPT, EXTRACTION_SYSTEM_PROMPT, SUBGRAPH_RELATIONSHIP_REFINE_PROMPT, RELATIONSHIP_MAPPING_TEXT
from utils.utils import config
from utils.example_selector import few_shot_prompt

from utils.vectorstore_config import get_vectorstore_and_client

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


VECTORSTORE_COLLECTION = config["vectorstore"]["index_name"]
VECTORSTORE_TEXT_KEY = config["vectorstore"]["text_key"]
WEAVIATE_HTTP_HOST = config["weaviate"]["http_host"]
WEAVIATE_HTTP_PORT = config["weaviate"]["http_port"]
WEAVIATE_HTTP_SECURE = config["weaviate"]["http_secure"]
WEAVIATE_GRPC_HOST = config["weaviate"]["grpc_host"]
WEAVIATE_GRPC_PORT = config["weaviate"]["grpc_port"]
WEAVIATE_GRPC_SECURE = config["weaviate"]["grpc_secure"]
WEAVIATE_API = config["weaviate"]["auth_api_key"]

NEO_4J_RUL = config["neo4j"]["url"]
NEO_4J_USERNAME = config["neo4j"]["username"]
NEO_4J_PASSWORD = config["neo4j"]["password"]
NEO_4J_DATABASE = config["neo4j"]["database"]

Chat_tongyi = config["llm"]["model"]
TEMPERATURE = config["llm"]["temperature"]

# Neo4j è¿æ¥å®ä¾‹
neo4j_graph = Neo4jGraph(
    url=NEO_4J_RUL,
    username=NEO_4J_USERNAME,
    password=NEO_4J_PASSWORD,
    database=NEO_4J_DATABASE
)

# 1. å­å›¾çš„åˆ†æå’Œåˆ†ç±»å‡½æ•°
async def analyze_and_route_subgraph_query(
    state: SubgraphState, *, config: RunnableConfig
) -> dict[str, SubRouter]:
    """
    åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶ç¡®å®šå…¶ç±»å‹ï¼š
    - attribute_query: éœ€è¦æŸ¥è¯¢æŸä¸ªG4ç»“æ„çš„å±æ€§
    - relationship_query: éœ€è¦æŸ¥è¯¢G4ç»“æ„ä¹‹é—´çš„å…³ç³»

    Returns a JSON response in the format:
    {
        "type": "attribute_query" | "relationship_query"
    }
    """
    # ä½¿ç”¨è¯Šæ–­ç‰ˆæœ¬çš„writerè·å–
    writer = get_stream_writer()
    if writer:
        writer({"subgraph_event": "å¼€å§‹åˆ†ææŸ¥è¯¢ç±»å‹"})

    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=True, api_key=api_key)
    messages = [
        {"role": "system", "content": SUBGRAPH_ROUTER_SYSTEM_PROMPT},
        {"role": "human", "content": state.question},
    ]

    logger.info("--- Subgraph: Analyze and Route Query ---")
    response = await model.ainvoke(messages)

    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†æµå¼è¾“å‡ºå†…å®¹
    raw_content = process_streaming_content(response.content)

    # å»æ‰ ```json ``` 
    cleaned_json = re.sub(r"^```json\s*|\s*```$", "", raw_content, flags=re.MULTILINE)

    # **ğŸš€ è§£æ JSON**
    try:
        response_data = json.loads(cleaned_json)  # âœ… è§£ææ¸…ç†åçš„ JSON
    except json.JSONDecodeError:
        logger.error(f"âŒ Failed to decode JSON after cleaning: {cleaned_json}")
        response_data = {"type": "attribute_query"}  # â— è§£æå¤±è´¥æ—¶è¿”å›é»˜è®¤å€¼ï¼Œé˜²æ­¢ NoneType é”™è¯¯

    # **ğŸš€ èµ‹å€¼åˆ° state**
    state.sub_router = response_data
    logger.info(f"âœ… Subgraph router response: {state.sub_router}")

    # ä»…åœ¨åˆ†æå®Œæˆåå‘é€ä¸€æ¬¡ç»“æœ
    if writer:
        writer({"subgraph_event": f"åˆ†æå®Œæˆ - ç¡®å®šä¸º{state.sub_router['type']}æŸ¥è¯¢"})

    return {"sub_router": state.sub_router}

# 2. å­å›¾çš„è·¯ç”±å‡½æ•°ï¼Œæ ¹æ®åˆ†ç±»ç»“æœå†³å®šè°ƒç”¨å“ªä¸ªæŸ¥è¯¢é“¾
def subgraph_route_query(state: SubgraphState) -> Literal["vector_search_chain", "extract_entities_and_relationships"]:
    """
    æ ¹æ®å­å›¾ä¸­çš„ sub_roter åˆ¤æ–­æŸ¥è¯¢ç±»å‹ï¼š
      - å¦‚æœ type ä¸º "attribute_query"ï¼Œåˆ™è¿”å› "vector_search_chain"
      - å¦‚æœ type ä¸º "relationship_query"ï¼Œåˆ™è¿”å› "extract_entities_and_relationships"
    """
    _type = state.sub_router["type"]
    
    # è¿™ä¸ªå‡½æ•°ä¸éœ€è¦å†æ¬¡å‘é€äº‹ä»¶ï¼Œå·²ç»åœ¨ä¸Šä¸€æ­¥å‘é€è¿‡äº†
    if _type == "attribute_query":
        return "vector_search_chain"
    elif _type == "relationship_query":
        return "extract_entities_and_relationships"
    else:
        raise ValueError(f"Unknown sub_router type: {_type}")

# 3. **å‘é‡æœç´¢æŸ¥è¯¢é“¾**
def vector_search_chain(state: SubgraphState) -> dict:
    """æ‰§è¡Œå‘é‡æœç´¢æŸ¥è¯¢ï¼ŒåŸºäºç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢è¿›è¡ŒåŒ¹é…"""
    
    # 1. è·å–ç”¨æˆ·é—®é¢˜
    query = state.question  # è·å–ä»ä¸»å›¾ä¼ é€’è¿‡æ¥çš„é—®é¢˜
    # 2. ä½¿ç”¨é…ç½®çš„Weaviateå‘é‡å­˜å‚¨å®ä¾‹æ‰§è¡ŒæŸ¥è¯¢
    vectorstore, client = get_vectorstore_and_client(index_name="KGDocument")  # åˆå§‹åŒ–Weaviateå‘é‡å­˜å‚¨å®ä¾‹å’Œå®¢æˆ·ç«¯
    result = vectorstore.similarity_search(query, k=1)  # è·å–æœ€ç›¸å…³çš„1ä¸ªç»“æœ
    # 3. å°†æŸ¥è¯¢ç»“æœå’Œç­”æ¡ˆæ¥æºæ·»åŠ åˆ°stateä¸­
    state.documents = []
    for doc in result:
        # æå–æ–‡æ¡£ä¸­çš„ 'page_content' å­—æ®µ
        text_content = doc.page_content  # ç›´æ¥ä½¿ç”¨page_contentå­—æ®µ
        state.documents.append({"content": text_content})

    writer = get_stream_writer()    
    # å¦‚æœæ²¡æœ‰ç»“æœï¼Œæ‰“å°æç¤ºï¼Œä»…å‘é€ä¸€æ¬¡äº‹ä»¶
    if writer:
        writer({"subgraph_event": f"å‘é‡æœç´¢å®Œæˆï¼Œæ‰¾åˆ°{len(state.documents)}æ¡ç»“æœ"})

    logger.info(f"Vector Search Result: {state.documents}")
    # è®¾ç½®ç­”æ¡ˆæ¥æºä¸º vector_search
    state.answer_source = ["vector_search"] * len(result)  # è®¾ç½®æ¥æºä¸º vector_search

    # è¿”å›åŒ…å«æ›´æ–°çš„stateä¿¡æ¯çš„å­—å…¸ï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨
    return {
        "documents": state.documents,
        "answer_source": state.answer_source,
    }

# 4. **å‘é‡+CYPHERæœç´¢æŸ¥è¯¢é“¾**
# ---------------- Step 1: å®ä½“ä¸å…³ç³»æå– ----------------
async def extract_entities_and_relationships(state: SubgraphState, *, config: RunnableConfig) -> dict:
    
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=False, api_key=api_key)
    messages = [
        {"role": "system", "content": EXTRACTION_SYSTEM_PROMPT},
        {"role": "human", "content": state.question},
    ]

    logger.info("--- Extract Entities and Relationships ---")
    response = await model.ainvoke(messages)

    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†æµå¼è¾“å‡ºå†…å®¹
    raw_content = process_streaming_content(response.content)

    # å»æ‰ ```json ```
    cleaned_json = re.sub(r"^```json\s*|\s*```$", "", raw_content, flags=re.MULTILINE)

    # **ç›´æ¥è§£æ JSONï¼Œä¸ä½¿ç”¨ isinstance**
    try:
        response_data = json.loads(cleaned_json)  # âœ… ç›´æ¥è§£æ
    except json.JSONDecodeError:
        logger.error(f"Failed to decode JSON: {raw_content}")
        response_data = {"entities": [], "relationships": []}  # è§£æå¤±è´¥è¿”å›é»˜è®¤å€¼

    # **ç›´æ¥èµ‹å€¼ List[str]**
    state.entities = response_data.get("entities", [])
    state.relationships = response_data.get("relationships", [])

    # ç®€åŒ–æ—¥å¿—è¾“å‡ºï¼Œåªè¾“å‡ºå®ä½“å’Œå…³ç³»çš„æ•°é‡
    logger.info(f"æ‰¾åˆ°å®ä½“æ•°é‡: {len(state.entities)}, å…³ç³»æ•°é‡: {len(state.relationships)}")

    writer = get_stream_writer()
    # å‘é€ç®€åŒ–ç»“æœ
    if writer:
       writer({"subgraph_event": f"æ‰¾åˆ°å®ä½“: {state.entities}, å…³ç³»: {state.relationships}"})

    return {
        "entities": state.entities,
        "relationships": state.relationships,
    }

# ---------------- Step 2: å®ä½“ç²¾ç‚¼ ----------------
async def refine_entities(state: SubgraphState, *, config: RunnableConfig) -> dict:

    vectorstore, client = get_vectorstore_and_client(index_name="KGDocument")
    refined_entities = []

    for entity in state.entities:
        entity_query = entity
        result = vectorstore.similarity_search(entity_query, k=1)

        if result and len(result) > 0:
            refined_entity = result[0].page_content
            refined_entities.append(refined_entity)
            # ä¸å†ä¸ºæ¯ä¸ªå®ä½“å‘é€æ¶ˆæ¯ï¼Œå‡å°‘äº‹ä»¶æ•°é‡
        else:
            refined_entities.append(entity)

    state.entities_refined = refined_entities
    logger.info(f"Refined entities: {state.entities_refined}")
    writer = get_stream_writer()
    # å‘é€ç²¾ç‚¼ç»“æœ - åªå‘é€ä¸€æ¬¡æ€»ç»“æ€§æ¶ˆæ¯
    if writer:
        writer({"subgraph_event": f"ç²¾ç‚¼åçš„å®ä½“: {state.entities_refined}"})
    return {
        "entities_refined": state.entities_refined,
    }

# ---------------- Step 3: å…³ç³»ç²¾ç‚¼ ----------------
async def refine_relationships(state: SubgraphState, *, config: RunnableConfig) -> dict:
    """
    ç»“åˆå®ä½“ç²¾ç‚¼åçš„ç»“æœå’Œé¢„å®šä¹‰çš„å…³ç³»æ˜ å°„å­—å…¸ï¼Œå¯¹ state.relationships è¿›è¡Œç²¾ç‚¼ï¼Œ
    ç›´æ¥ä½¿ç”¨ LLM ç”Ÿæˆæœ€ç»ˆçš„å…³ç³»åç§°ï¼Œæ›´æ–° state.relationships_refinedã€‚
    """

    prompt_text = SUBGRAPH_RELATIONSHIP_REFINE_PROMPT.format(
        rel_dict=RELATIONSHIP_MAPPING_TEXT,
        question=state.question,
        refined_entities=state.entities_refined,
        original_relationships=state.relationships
    )

    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=False, api_key=api_key)

    # **è°ƒç”¨ LLM è®©å…¶è¿”å›å•ä¸ªå…³ç³»åç§°ï¼Œç¡®ä¿åŒ…å«userè§’è‰²çš„æ¶ˆæ¯**
    response = await model.ainvoke([
        {"role": "system", "content": prompt_text},
        {"role": "user", "content": "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç¡®å®šæœ€åˆé€‚çš„å…³ç³»åç§°ã€‚"}
    ])

    # **ç›´æ¥ä½¿ç”¨ response.content ä½œä¸ºå…³ç³»åç§°**
    refined_relationship = process_streaming_content(response.content) if response.content else "unknown"
    refined_relationship = refined_relationship.strip()

    # **æ›´æ–°çŠ¶æ€**
    state.relationships_refined = [refined_relationship]  # âœ… ç›´æ¥å­˜å…¥åˆ—è¡¨
    logger.info(f"LLM Returned Relationship: {refined_relationship}")  # æ–¹ä¾¿è°ƒè¯•
    logger.info(f"Refined relationships: {state.relationships_refined}")

    writer = get_stream_writer()
    # ç®€åŒ–è¾“å‡º
    if writer:
        writer({"subgraph_event": f"ç²¾ç‚¼åçš„å…³ç³»: {state.relationships_refined}"})

    return {
        "relationships_refined": state.relationships_refined
    }

# ---------------- Step 4: ç”Ÿæˆç²¾ç¡®çš„ Cypher æŸ¥è¯¢ ----------------
def _cleanup_cypher_code_fences(text: str) -> str:
    """
    å…ˆå»æ‰ä¸‰é‡åå¼•å·ä»¥åŠç±»ä¼¼ ```cypher çš„å—æ ‡è®°ï¼Œåªä¿ç•™å†…éƒ¨çš„ Cypher è¯­å¥ã€‚
    è¿™æ ·å¦‚æœ LLM è¿”å›:
    ```cypher
    MATCH ...
    RETURN ...
    ```
    æˆ‘ä»¬å°±èƒ½æŠŠä¸‰é‡åå¼•å·å»æ‰ï¼Œåªç•™çº¯æ–‡æœ¬ MATCH ... RETURN ...
    """

    # 1) å»é™¤å½¢å¦‚ ```cypher æˆ– ```bash ç­‰å¯èƒ½çš„è¯­è¨€æ ‡è¯†
    #    è¿™é‡Œç”¨æ­£åˆ™æ•è·ä¸‰é‡åå¼•å·åŠ å¯é€‰å•è¯å­—ç¬¦
    text = re.sub(r"```[a-zA-Z_-]*", "", text)

    # 2) å†å»é™¤ä»»ä½•å‰©ä½™çš„ä¸‰é‡åå¼•å· ```
    text = text.replace("```", "")

    # æœ€å strip ä¸€ä¸‹
    return text.strip()


def _remove_enclosing_quotes(text: str) -> str:
    """åªå»æ‰æœ€å‰é¢å’Œæœ€åé¢æˆå¯¹å‡ºç°çš„å¼•å·ï¼ˆæˆ–ä¸‰é‡å¼•å·ï¼‰ï¼Œä¸åŠ¨å†…éƒ¨å†…å®¹ã€‚"""
    text = text.strip()

    # 1) å¦‚æœæ˜¯ä¸‰é‡åŒå¼•å·åŒ…è£¹
    if len(text) >= 6 and text.startswith('"""') and text.endswith('"""'):
        return text[3:-3].strip()

    # 2) å¦‚æœæ˜¯ä¸‰é‡å•å¼•å·åŒ…è£¹
    if len(text) >= 6 and text.startswith("'''") and text.endswith("'''"):
        return text[3:-3].strip()

    # 3) å¦‚æœæ˜¯å•ä¸ªåŒå¼•å·åŒ…è£¹
    if len(text) >= 2 and text.startswith('"') and text.endswith('"'):
        return text[1:-1].strip()

    # 4) å¦‚æœæ˜¯å•ä¸ªå•å¼•å·åŒ…è£¹
    if len(text) >= 2 and text.startswith("'") and text.endswith("'"):
        return text[1:-1].strip()

    # å¦åˆ™åŸæ ·è¿”å›
    return text


async def generate_cypher_query(state: SubgraphState, *, config: RunnableConfig) -> dict:
    """
    åˆ©ç”¨ KG schemaã€ç”¨æˆ·æŸ¥è¯¢ã€ç²¾ç‚¼åçš„å®ä½“å’Œå…³ç³»ä¿¡æ¯ç”Ÿæˆç²¾ç¡®çš„ Cypher æŸ¥è¯¢è¯­å¥ï¼Œ
    æ›´æ–° state.cypher_queryã€‚
    """

    # 1. ç”¨ FewShotPromptTemplate æ¸²æŸ“ Prompt
    prompt = few_shot_prompt.format(
        schema=state.schema,
        question=state.question,
        refined_entities=", ".join(state.entities_refined),
        refined_relationships=", ".join(state.relationships_refined)
    )

    # 2. è°ƒç”¨ LLM è·å–å›å¤ï¼Œç¡®ä¿åŒ…å«userè§’è‰²çš„æ¶ˆæ¯
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=False, api_key=api_key)
    response = await model.ainvoke([
        {"role": "system", "content": prompt},
        {"role": "user", "content": "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸€ä¸ªåˆé€‚çš„CypheræŸ¥è¯¢è¯­å¥ã€‚"}
    ])

    # 3. åŸå§‹çš„ LLM è¾“å‡ºï¼Œç¡®ä¿å¤„ç†æµå¼è¾“å‡º
    raw_cypher = process_streaming_content(response.content)

    # 4. å»æ‰ä¸‰é‡åå¼•å·ä»£ç å— (```cypher ...```)
    without_fences = _cleanup_cypher_code_fences(raw_cypher)

    # 5. å»æ‰æœ€å¤–å±‚å¼•å·
    cleaned_cypher = _remove_enclosing_quotes(without_fences)

    # 6. å­˜åˆ° state å¹¶æ—¥å¿—è®°å½•
    state.cypher_query = cleaned_cypher
    state.answer_source = ["cypher_search"]  # è®¾ç½®æ¥æºä¸º cypher_search
    logger.info(f"Generated Cypher Query: {state.cypher_query}")

    writer = get_stream_writer()
    # å‘é€æŸ¥è¯¢ç»“æœ - åªå‘é€ä¸€æ¬¡æ€»ç»“æ¶ˆæ¯
    if writer:
        writer({"subgraph_event": f"CypheræŸ¥è¯¢ç”Ÿæˆå®Œæˆ: {state.cypher_query}"})

    return {
        "cypher_query": state.cypher_query,
        "answer_source": state.answer_source,
    }

# ---------------- Step 5: æ‰§è¡Œ Cypher æŸ¥è¯¢ ----------------
async def execute_cypher_query(state: SubgraphState, *, config: RunnableConfig) -> dict:

    # è·å–æµå¼å†™å…¥å™¨
    writer = get_stream_writer()
    # ç›´æ¥æ‰§è¡Œæ•´æ¡æŸ¥è¯¢
    results = neo4j_graph.query(state.cypher_query)

    state.documents = results
    logging.info(f"Executed Cypher Query: {state.cypher_query}")
    logging.info(f"Query Results: {results}")

    # å‘é€æ‰§è¡Œç»“æœ - ä»…å‘é€ä¸€æ¬¡æ€»ç»“
    if writer:
        writer({"subgraph_event": f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆ: æ‰¾åˆ°{len(results)}æ¡ç»“æœ"})
            
    return {
        "documents": state.documents,
        "cypher_query": state.cypher_query,
    }


subgraph_checkpointer = checkpointer

# 3. **å­å›¾æ„å»ºå’ŒèŠ‚ç‚¹å®šä¹‰**
builder = StateGraph(SubgraphState)

# 1. **æ·»åŠ èŠ‚ç‚¹**
builder.add_node(analyze_and_route_subgraph_query)
builder.add_node(vector_search_chain)
builder.add_node(extract_entities_and_relationships)
builder.add_node(refine_entities)
builder.add_node(refine_relationships)
builder.add_node(generate_cypher_query)
builder.add_node(execute_cypher_query)

# 2. **æ·»åŠ è¾¹**
builder.add_edge(START, "analyze_and_route_subgraph_query")
builder.add_conditional_edges("analyze_and_route_subgraph_query", subgraph_route_query)
builder.add_edge("vector_search_chain", END)

builder.add_edge("extract_entities_and_relationships", "refine_entities")
builder.add_edge("refine_entities", "refine_relationships")
builder.add_edge("refine_relationships", "generate_cypher_query")
builder.add_edge("generate_cypher_query","execute_cypher_query")
builder.add_edge("execute_cypher_query", END)

# 3. **ç¼–è¯‘å›¾**
search_graph = builder.compile(checkpointer=subgraph_checkpointer)

