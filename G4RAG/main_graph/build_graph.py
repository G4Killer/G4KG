import json
import re
from typing import Literal, cast, Any

from jinja2 import Template
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph, END, START
from utils.utils import checkpointer, process_streaming_content
import logging
from sub_graph.subgraph_states import SubgraphState
from utils.utils import config
from prompts.prompt_templates import ROUTER_SYSTEM_PROMPT, MORE_INFO_SYSTEM_PROMPT, GENERAL_SYSTEM_PROMPT, \
    RESPONSE_SYSTEM_PROMPT, REWRITE_QUERY_PROMPT, G4KG_GENERAL_SYSTEM_PROMPT
from src.chat_llm import ChatTongyiLLM, build_tongyi_llm, get_api_key_from_config
from main_graph.graph_states import AgentState, Router, InputState
from sub_graph.build_subgraph import search_graph
from langgraph.config import get_stream_writer

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å‡è®¾ä»é…ç½®æ–‡ä»¶ä¸­è·å–æ¨¡å‹ä¿¡æ¯
Chat_tongyi = config["llm"]["model"]
#CUSTOM_EMBEDDING_MODEL = config["llm"]["custom_embedding_model"]
TEMPERATURE = config["llm"]["temperature"]


# 1. **åˆ†æå¹¶è·¯ç”±ç”¨æˆ·æŸ¥è¯¢ (`analyze_and_route_query`)**
async def analyze_and_route_query(state: AgentState, *, config: RunnableConfig) -> dict[str, Router]:
    """
    Analyze the user's query and determine its category:
    - KG_query: Requires information retrieval from the Knowledge Graph (KG).
    - more-info: The user's input is incomplete and requires additional details.
    - general: The query is not related to the Knowledge Graph.

    Returns a JSON response in the format:
    {
        "logic": "Reason for classification",
        "type": "KG_query" | "more-info" | "general"
    }
    """
    # è·å–ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹
    user_query = ""
    for msg in reversed(state.messages):
        if isinstance(msg, dict) and msg.get("role") == "user" and "content" in msg:
            user_query = msg.get("content", "")
            break
        elif hasattr(msg, "content") and getattr(msg, "role", None) == "user":
            user_query = msg.content
            break
    
    # è·å–æµå¼å†™å…¥å™¨
    writer = get_stream_writer()
    if writer:
        writer({"maingraph_event": "å¼€å§‹åˆ†æç”¨æˆ·æŸ¥è¯¢ç±»å‹..."})
    
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=True, api_key=api_key)
    template = Template(ROUTER_SYSTEM_PROMPT)
    prompt = template.render(schema=state.schema)
    messages = [{"role": "system", "content": prompt}] + state.messages

    logging.info("--- Analyzing and Routing Query ---")

    response = await model.ainvoke(messages)

    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†æµå¼è¾“å‡ºå†…å®¹
    raw_content = process_streaming_content(response.content)

    # Handle Markdown-style JSON blocks and unexpected whitespace
    cleaned_json = re.sub(r"^\s*json\s*", "", raw_content, flags=re.MULTILINE)  # Remove leading `json\n`
    cleaned_json = re.sub(r"^```json\s*|\s*```$", "", cleaned_json, flags=re.MULTILINE)  # Remove ` ```json ` blocks
    cleaned_json = cleaned_json.strip()  # Final cleanup

    # JSON Parsing with Fallback
    try:
        response_data = json.loads(cleaned_json)  # Parse cleaned JSON
    except json.JSONDecodeError:
        logging.error(f"âŒ JSON decoding failed, returning default classification: {cleaned_json}")
        response_data = {"logic": "Unable to parse response, default classification", "type": "general"}
        # æµå¼å†™å…¥è§£æå¤±è´¥æ¶ˆæ¯
        if writer:
            try:
                writer({"maingraph_event": "è·¯ç”±å†³ç­–è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»"})
            except:
                pass

    # Assign to state
    state.router = response_data
    logging.info(f"âœ… Router Response: {state.router}")
    # æµå¼å†™å…¥è·¯ç”±å†³ç­–ç»“æœ
    if writer:
        router_type = state.router.get("type", "æœªçŸ¥")
        # æ˜¾ç¤ºè·¯ç”±ç±»å‹ã€æ‘˜è¦é€»è¾‘å’Œç”¨æˆ·æŸ¥è¯¢
        writer({"maingraph_event": f"è·¯ç”±å†³ç­–: ç¡®å®šæŸ¥è¯¢ç±»å‹ä¸º{router_type}"})

    return {"router": state.router}

# 2. **æŸ¥è¯¢è·¯ç”± (`route_query`)**
def route_query(state: AgentState) -> Literal["rewrite_query", "ask_for_more_info", "respond_to_general_query", "respond_to_g4kg_general_query"]:
    """æ ¹æ®æŸ¥è¯¢åˆ†ç±»å†³å®šæ¥ä¸‹æ¥çš„æ­¥éª¤"""
    _type = state.router["type"]
    if _type == "KG_query":
        return "rewrite_query"
    elif _type == "more-info":
        return "ask_for_more_info"
    elif _type == "general":
        return "respond_to_general_query"
    elif _type == "G4KG-General":
        return "respond_to_g4kg_general_query"
    else:
        raise ValueError(f"Unknown router type {_type}")

async def rewrite_query(state: AgentState, *, config: RunnableConfig) -> dict:
    """
    é‡æ–°ç”Ÿæˆæ¸…æ™°çš„é—®é¢˜ï¼Œä»¥ä¾¿ä¼ é€’åˆ°å­å›¾ï¼Œç¡®ä¿å®Œæ•´çš„ä¸Šä¸‹æ–‡ã€‚
    """
    # è·å–æµå¼å†™å…¥å™¨
    writer = get_stream_writer()
    if writer:
        writer({"maingraph_event": "ç”Ÿæˆæœ€ç»ˆå›ç­”: å¤„ç†æŸ¥è¯¢ç»“æœå¹¶ç”Ÿæˆå›å¤"})

    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=False, api_key=api_key)
    prompt = REWRITE_QUERY_PROMPT.format(context=state.messages)
    
    # è·å–ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²
    user_query = ""
    for msg in reversed(state.messages):
        if isinstance(msg, dict) and msg.get("role") == "user" and "content" in msg:
            user_query = msg.get("content", "")
            break
        elif hasattr(msg, "content") and getattr(msg, "role", None) == "user":
            user_query = msg.content
            break
    
    # ç¡®ä¿åŒ…å«ä¸€ä¸ªuserè§’è‰²çš„æ¶ˆæ¯
    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": user_query or "è¯·é‡å†™ä¸Šè¿°æŸ¥è¯¢ï¼Œä½¿å…¶æ›´æ¸…æ™°"}
    ]
    
    response = await model.ainvoke(messages)
    
    # ä½¿ç”¨å…¬å…±æ–¹æ³•å¤„ç†å“åº”å†…å®¹
    rewritten_question = process_streaming_content(response.content)
    logging.info(f"ğŸ”„ Rewritten Query: {rewritten_question}")

    return {"processed_query": rewritten_question}


async def conduct_search(state: AgentState, *, config: RunnableConfig) -> dict[str, Any]:
    """
    è°ƒç”¨å­å›¾è¿›è¡ŒæŸ¥è¯¢ï¼Œå¹¶è¿”å›æŸ¥è¯¢ç»“æœ
    """
    # è·å–æµå¼å†™å…¥å™¨
    writer = get_stream_writer()
    if writer:
        writer({"maingraph_event": "å¼€å§‹å›¾è°±æŸ¥è¯¢..."})
    logging.info("ğŸ” Calling subgraph...")
    logging.debug(f"ä½¿ç”¨æŸ¥è¯¢: {state.processed_query}")
    
    # æ·»åŠ æ›´å¤šæ—¥å¿—ä»¥ä¾¿è°ƒè¯•
    logging.debug(f"ä½¿ç”¨é…ç½®: {config}")

    # è®©å­å›¾ç»§æ‰¿ä¸»å›¾çš„ `Checkpoint`
    subgraph_config = config  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ `config`ï¼Œä¸è¦æ‰‹åŠ¨åˆ›å»ºæ–° `thread_id`

    # æ‰§è¡Œå­å›¾æŸ¥è¯¢ï¼Œä½¿ç”¨astreamè€Œä¸æ˜¯ainvokeï¼Œä»¥æ”¯æŒäº‹ä»¶æµ
    last_result = None
    subgraph_received = False
    
    # æ”¶é›†å­å›¾äº‹ä»¶
    async for event in search_graph.astream(
            SubgraphState(question=state.processed_query),
            config=subgraph_config,  # ä¼ é€’ç›¸åŒçš„ configï¼Œç¡®ä¿ memory å…±äº«
            stream_mode="custom"  # ä½¿ç”¨customæ¨¡å¼ä»¥æ”¯æŒå®æ—¶äº‹ä»¶ä¼ é€’
    ):
        # è®°å½•æ”¶åˆ°çš„æ¯ä¸ªäº‹ä»¶ä»¥è¾…åŠ©è°ƒè¯•
        logging.debug(f"æ”¶åˆ°å­å›¾äº‹ä»¶: {event}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å­å›¾è‡ªå®šä¹‰äº‹ä»¶
        if isinstance(event, dict) and "subgraph_event" in event:
            subgraph_received = True
            logging.info(f"æ”¶åˆ°å­å›¾æ¨ç†äº‹ä»¶: {event['subgraph_event']}")
            # è¿™äº›äº‹ä»¶ä¼šè‡ªåŠ¨é€šè¿‡stream_modeä¼ é€’ç»™WebSocketæœåŠ¡å™¨
            continue
            
        # ä¿å­˜æœ€åä¸€ä¸ªç»“æœç”¨äºè¿”å›
        last_result = event

    # è®°å½•å­å›¾çš„ Checkpointï¼Œå¹¶ç”¨å®ƒå›å¡«ç¼ºå¤±çš„æµå¼ç»“æœ
    checkpoint_data = checkpointer.get(subgraph_config)
    logging.info(f"ğŸ“Œ Subgraph Checkpoint: {checkpoint_data}")
    channel_values = checkpoint_data.get("channel_values", {}) if isinstance(checkpoint_data, dict) else {}

    # æ²¡æœ‰é€šè¿‡æµäº‹ä»¶æ‹¿åˆ°ç»“æœæ—¶ï¼Œä¼˜å…ˆå°è¯•ä»checkpointå›å¡«
    if not last_result:
        if channel_values:
            logging.warning("å­å›¾æµäº‹ä»¶ç¼ºå°‘æœ€ç»ˆç»“æœï¼Œä½¿ç”¨ checkpoint å›å¡«")
            last_result = channel_values
        else:
            logging.warning("å­å›¾æœªè¿”å›ä»»ä½•ç»“æœ")
            last_result = {}
            # æµå¼å†™å…¥å­å›¾ç»“æœä¸ºç©ºçš„æ¶ˆæ¯
            if writer:
                writer({"maingraph_event": "å­å›¾æœªè¿”å›ç»“æœ"})
    else:
        # å¦‚æœæµäº‹ä»¶ç¼ºå°‘éƒ¨åˆ†å­—æ®µï¼Œç”¨checkpointè¡¥å…¨
        if isinstance(last_result, dict) and channel_values:
            for key, value in channel_values.items():
                last_result.setdefault(key, value)
    
    # å°†subgraph_receivedæ£€æŸ¥æ”¾åœ¨å¾ªç¯åï¼Œé¿å…é”™è¯¯çš„"æœªæ”¶åˆ°å­å›¾äº‹ä»¶"è­¦å‘Š
    # åªåœ¨DEBUGçº§åˆ«è®°å½•è¿™ä¸ªä¿¡æ¯ï¼Œé¿å…ä½œä¸ºé”™è¯¯æ˜¾ç¤º
    if not subgraph_received:
        logging.debug("å­å›¾è¯Šæ–­: æœªæ”¶åˆ°ä»»ä½•å­å›¾äº‹ä»¶æµï¼Œå¯èƒ½stream_modeé…ç½®é—®é¢˜ï¼Œä½†å­å›¾å¤„ç†å·²å®Œæˆ")
        # ä¸å†å‘å‰ç«¯å‘é€æ­¤è­¦å‘Šï¼Œå› ä¸ºè¿™æ˜¯å†…éƒ¨è¯Šæ–­ä¿¡æ¯ï¼Œä¸å½±å“æ­£å¸¸ç»“æœ

    # å¤„ç†æŸ¥è¯¢ç»“æœ
    docs = last_result.get("documents", [])
    cypher = last_result.get("cypher_query", "")
    state.documents = docs
    state.answer_source = last_result.get("answer_source", [])
    state.cypher_query = cypher
    logging.info(f"Cypher query executed: {state.cypher_query}")
    
    # æ·»åŠ å­å›¾å¤„ç†å®Œæˆçš„ä¿¡æ¯
    logging.info(f"å­å›¾å¤„ç†å®Œæˆï¼Œæ–‡æ¡£æ•°é‡: {len(docs)}")

    # ç¡®ä¿ `messages` è¢«è¿½åŠ åˆ° `state.messages`
    if "messages" in last_result:
        state.messages.extend(last_result["messages"])

    return {
        "documents": state.documents,
        "answer_source": state.answer_source,
        "cypher_query": state.cypher_query
    }

# 3. **ç”Ÿæˆè¯·æ±‚æ›´å¤šä¿¡æ¯çš„å›ç­” (`ask_for_more_info`)**
async def ask_for_more_info(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[BaseMessage]]:
    """ç”Ÿæˆè¯¢é—®ç”¨æˆ·æ›´å¤šä¿¡æ¯çš„å“åº”"""
        
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=True, api_key=api_key)
    system_prompt = MORE_INFO_SYSTEM_PROMPT.format(logic=state.router["logic"])
    messages = [{"role": "system", "content": system_prompt}] + state.messages
    
    logging.info("---ASKING FOR MORE INFORMATION---")
    
    response = await model.ainvoke(messages)
    
    # æ³¨æ„ï¼šè¿™é‡Œä¸å¤„ç†response.contentï¼Œç›´æ¥è¿”å›responseå¯¹è±¡
    # å› ä¸ºåç»­å¤„ç†ä¼šè‡ªåŠ¨å¤„ç†BaseMessageå¯¹è±¡
    return {"messages": [response]}

# 5. **æ™®é€šé—®é¢˜å“åº” (`respond_to_general_query`)**
async def respond_to_general_query(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[BaseMessage]]:
    """ç”Ÿæˆæ™®é€šæŸ¥è¯¢çš„å›ç­”"""
        
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=True, api_key=api_key)
    system_prompt = GENERAL_SYSTEM_PROMPT.format(logic=state.router["logic"])
    logging.info("---GENERATE GENERAL RESPONSE---")
    messages = [{"role": "system", "content": system_prompt}] + state.messages
    
    logging.info(f"General Query Messages: {messages}")
            
    response = await model.ainvoke(messages)
    
    # è¾“å‡ºæ—¥å¿—ä½†ä¸ä¿®æ”¹responseå¯¹è±¡
    logging.info(f"General Response Type: {type(response)}")
    if hasattr(response, 'content'):
        logging.info(f"General Response Content Type: {type(response.content)}")
    
    # ç›´æ¥è¿”å›responseå¯¹è±¡ï¼Œä¸å¤„ç†content
    return {"messages": [response]}

# æ–°å¢G4KGé€šç”¨é—®é¢˜å›ç­”å‡½æ•°
async def respond_to_g4kg_general_query(
    state: AgentState, *, config: RunnableConfig
) -> dict[str, list[BaseMessage]]:
    """ç”Ÿæˆå…³äºG4KGæ•°æ®åº“çš„é€šç”¨é—®é¢˜å›ç­”"""
        
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=True, api_key=api_key)
    system_prompt = G4KG_GENERAL_SYSTEM_PROMPT.format(logic=state.router["logic"], schema=state.schema)
    logging.info("---GENERATE G4KG GENERAL RESPONSE---")
    messages = [{"role": "system", "content": system_prompt}] + state.messages
    
    logging.info(f"G4KG General Query Messages: {messages}")
            
    response = await model.ainvoke(messages)
    
    # è¾“å‡ºæ—¥å¿—ä½†ä¸ä¿®æ”¹responseå¯¹è±¡
    logging.info(f"G4KG General Response Type: {type(response)}")
    if hasattr(response, 'content'):
        logging.info(f"G4KG General Response Content Type: {type(response.content)}")
    
    # ç›´æ¥è¿”å›responseå¯¹è±¡ï¼Œä¸å¤„ç†content
    return {"messages": [response]}

async def respond(state: AgentState, *, config: RunnableConfig) -> dict[str, list[BaseMessage]]:
    """
    ç”Ÿæˆæœ€åå›ç­”ï¼Œä¸å†åšä»»ä½•ç‰¹å®šå­—æ®µè§£æï¼Œè€Œæ˜¯æŠŠ documents é‡Œçš„å­—å…¸åŸæ · JSON åºåˆ—åŒ–
    """
    logging.info("--- RESPONSE GENERATION STEP ---")
        
    api_key = get_api_key_from_config(config)
    model = build_tongyi_llm(model=Chat_tongyi, temperature=TEMPERATURE, streaming=True, api_key=api_key)

    # 1. æ„é€ å¯è¯»æ–‡æ¡£å­—ç¬¦ä¸²
    try:
        # ç¡®ä¿documentsæœ‰æ•ˆ
        documents = state.documents if state.documents else []
        
        # é‡æ–°æ ¼å¼åŒ– contextï¼Œä½¿å…¶æ›´å®¹æ˜“è§£æ
        readable_context = "\n".join(
            [f"[{i + 1}] {json.dumps(doc, ensure_ascii=False)}" for i, doc in enumerate(documents)]
        )
        
        if not readable_context.strip():
            readable_context = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
            
        logging.info(f"Readable context prepared, length: {len(readable_context)}")
    except Exception as e:
        logging.error(f"æ„é€ æ–‡æ¡£å­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}")
        readable_context = "å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™"
    # 2. æ”¾åˆ° Prompt
    try:
        template = Template(RESPONSE_SYSTEM_PROMPT)
        cypher_query = state.cypher_query if state.cypher_query else ""
        prompt = template.render(context=readable_context, cypher=cypher_query)
        messages = [{"role": "system", "content": prompt}] + state.messages
    except Exception as e:
        logging.error(f"æ„å»ºæç¤ºæ—¶å‡ºé”™: {str(e)}")
        # ä½¿ç”¨ç®€å•æç¤ºä½œä¸ºåå¤‡
        messages = [{"role": "system", "content": "è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜æä¾›æœ‰å¸®åŠ©çš„å›ç­”ã€‚"}] + state.messages

    # 3. è°ƒç”¨ LLM
    logging.info(f"Invoking LLM for final response")

    response = await model.ainvoke(messages)
    
    # è®°å½•å“åº”ç±»å‹ä¿¡æ¯ä½†ä¸ä¿®æ”¹å¯¹è±¡
    logging.info(f"Final Response Type: {type(response)}")
    if hasattr(response, 'content'):
        logging.info(f"Final Response Content Type: {type(response.content)}")
    
    return {"messages": [response]}

# 6. **çŠ¶æ€å›¾æ„å»º**

builder = StateGraph(AgentState, input=InputState)
builder.add_node(analyze_and_route_query)
builder.add_edge(START, "analyze_and_route_query")
builder.add_conditional_edges("analyze_and_route_query", route_query)

builder.add_node(conduct_search)
builder.add_node(ask_for_more_info)
builder.add_node(respond_to_general_query)
builder.add_node(respond_to_g4kg_general_query)
builder.add_node(rewrite_query)
builder.add_node("respond", respond)

builder.add_edge("rewrite_query","conduct_search")
builder.add_edge("conduct_search", "respond")
builder.add_edge("respond", END)
builder.add_edge("respond_to_g4kg_general_query", END)
builder.add_edge("respond_to_general_query", END)
builder.add_edge("ask_for_more_info", END)

# é€šè¿‡`builder.compile()`ç¼–è¯‘å›¾
graph = builder.compile(checkpointer=checkpointer)
# æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥å¹¶æ‰§è¡Œ

